[{"authors":null,"categories":null,"content":"Duy Nhat Phan is currently a postdoc at the Department of Mathematics and Statistics, University of Massachusetts Lowell. He obtained a Ph.D. in numerical optimization at the University of Lorraine, France. After that, he worked there as a Research Fellow for two years. Then, he worked for one year and a half at the Dynamic Decision-Making laboratory at Carnegie Mellon University.\n  Curriculum Vitae,  Google Scholar,  Github.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://nhatpd.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Duy Nhat Phan is currently a postdoc at the Department of Mathematics and Statistics, University of Massachusetts Lowell. He obtained a Ph.D. in numerical optimization at the University of Lorraine, France.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"","date":1670630400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1670630400,"objectID":"646d89ced581716626c192e32c3ae300","permalink":"https://nhatpd.github.io/courses/math2210/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/courses/math2210/","section":"courses","summary":"","tags":null,"title":"MATH.2210: Linear Algebra I, UMass Lowell (Fall 2022, Spring 2023)","type":"book"},{"authors":["Le Thi Khanh Hien","Duy Nhat Phan\"","Nicolas Gillis"],"categories":null,"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669852800,"objectID":"fc1be2fc1407e4964e4462b2bc326319","permalink":"https://nhatpd.github.io/publication/phien_jmlr_2020/","publishdate":"2022-12-01T20:13:52.623034Z","relpermalink":"/publication/phien_jmlr_2020/","section":"publication","summary":"In this paper, we introduce TITAN, a novel inerTIal block majorizaTion minimizAtioN framework for non-smooth non-convex optimization problems. To the best of our knowledge, TITAN is the first framework of block-coordinate update method that relies on the majorization-minimization framework while embedding inertial force to each step of the block updates. The inertial force is obtained via an extrapolation operator that subsumes heavy-ball and Nesterov-type accelerations for block proximal gradient methods as special cases. By choosing various surrogate functions, such as proximal, Lipschitz gradient, Bregman, quadratic, and composite surrogate functions, and by varying the extrapolation operator, TITAN produces a rich set of inertial block-coordinate update methods. We study sub-sequential convergence as well as global convergence for the generated sequence of TITAN. We illustrate the effectiveness of TITAN on two important machine learning problems, namely sparse non-negative matrix factorization and matrix completion.","tags":null,"title":"Inertial block majorization minimization framework for nonconvex nonsmooth optimization","type":"publication"},{"authors":["Le Thi Khanh Hien","Duy Nhat Phan","Nicolas Gillis"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"dc7bf22593625205e2d1e5029da278fb","permalink":"https://nhatpd.github.io/publication/jhien_coa_2022/","publishdate":"2022-07-05T20:13:52.626042Z","relpermalink":"/publication/jhien_coa_2022/","section":"publication","summary":"In this paper, we propose an algorithmic framework, dubbed inertial alternating direction methods of multipliers (iADMM), for solving a class of nonconvex nonsmooth multiblock composite optimization problems with linear constraints. Our framework employs the general minimization-majorization (MM) principle to update each block of variables so as to not only unify the convergence analysis of previous ADMM that use specific surrogate functions in the MM step, but also lead to new efficient ADMM schemes. To the best of our knowledge, in the nonconvex nonsmooth setting, ADMM used in combination with the MM principle to update each block of variables, and ADMM combined with inertial terms for the primal variables have not been studied in the literature. Under standard assumptions, we prove the subsequential convergence and global convergence for the generated sequence of iterates. We illustrate the effectiveness of iADMM on a class of nonconvex low-rank representation problems.","tags":null,"title":"Inertial alternating direction method of multipliers for non-convex non-smooth optimization","type":"publication"},{"authors":["Nguyen Thuy Ngoc","Duy Nhat Phan","Cleotilde Gonzalez"],"categories":null,"content":"","date":1656460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656460800,"objectID":"06ff7b9ba03b8e46ff3728c2fbdcf235","permalink":"https://nhatpd.github.io/publication/jngoc_brm_2022/","publishdate":"2022-06-05T20:13:52.626042Z","relpermalink":"/publication/jngoc_brm_2022/","section":"publication","summary":"Instance-based learning theory (IBLT) is a comprehensive account of how humans make decisions from experience during dynamic tasks. Since it was first proposed almost two decades ago, multiple computational models have been constructed based on IBLT (i.e., IBL models). These models have been demonstrated to be very successful in explaining and predicting human decisions in multiple decision-making contexts. However, as IBLT has evolved, the initial description of the theory has become less precise, and it is unclear how its demonstration can be expanded to more complex, dynamic, and multi-agent environments. This paper presents an updated version of the current theoretical components of IBLT in a comprehensive and precise form. It also provides an advanced implementation of the full set of theoretical mechanisms, SpeedyIBL, to unlock the capabilities of IBLT to handle a diverse taxonomy of individual and multi-agent decision-making problems. SpeedyIBL addresses a practical computational issue in past implementations of IBL models, the curse of exponential growth, that emerges from memory-based tabular computations. When more observations accumulate over time, there is an exponential growth of the memory of instances that leads directly to an exponential slowdown of the computational time. Thus, SpeedyIBL leverages parallel computation with vectorization to speed up the execution time of IBL models. We evaluate the robustness of SpeedyIBL over an existing implementation of IBLT in decision games of increased complexity. The results not only demonstrate the applicability of IBLT through a wide range of decision-making tasks, but also highlight the improvement of SpeedyIBL over its prior implementation as the complexity of decision features the of agents increase. The library is open sourced for the use of the broad research community.","tags":null,"title":"SpeedyIBL: A comprehensive, precise, and fast implementation of instance-based learning theory","type":"publication"},{"authors":["Le Thi Khanh Hien","Duy Nhat Phan","Nicolas Gillis","Masoud Ahookhosh","and Panagiotis Patrinos"],"categories":null,"content":"","date":1646511232,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646511232,"objectID":"cec9bf9fed02348803591cbfbdc593bc","permalink":"https://nhatpd.github.io/publication/jhien_siam_2022/","publishdate":"2022-03-05T20:13:52.626042Z","relpermalink":"/publication/jhien_siam_2022/","section":"publication","summary":"In this paper, we consider a class of nonsmooth nonconvex optimization problems whose objective is the sum of a block relative smooth function and a proper and lower semicontinuous block separable function. Although the analysis of block proximal gradient (BPG) methods for the class of block L-smooth functions has been successfully extended to Bregman BPG methods that deal with the class of block relative smooth functions, accelerated Bregman BPG methods are scarce and challenging to design. Taking our inspiration from Nesterov-type acceleration and the majorization-minimization scheme, we propose a block alternating Bregman majorization minimization framework with extrapolation (BMME). We prove subsequential convergence of BMME to a first-order stationary point under mild assumptions and study its global convergence under stronger conditions. We illustrate the effectiveness of BMME on the penalized orthogonal nonnegative matrix factorization problem.","tags":null,"title":"Block Bregman Majorization Minimization with Extrapolation","type":"publication"},{"authors":["Hoai An Le Thi","Duy Nhat Phan","Tao Pham Dinh"],"categories":null,"content":"","date":1641413632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641413632,"objectID":"d0bc7c6881ba6d0775972449ed6f9b95","permalink":"https://nhatpd.github.io/publication/jan_neuro_2022/","publishdate":"2022-01-05T20:13:52.626042Z","relpermalink":"/publication/jan_neuro_2022/","section":"publication","summary":"Variable selection plays an important role in analyzing high dimensional data and is a fundamental problem in machine learning. When the data possesses certain group structures in which individual variables are also meaningful scientifically, we are naturally interested in selecting important groups as well as important variables within the selected groups. This is referred to as the bi-level variable selection which is much more complex than the selection of individual variables. In recent years, research on the topic of variable selection is very active, but the majority of the work is focused on the individual variable selection. There is therefore a need to further develop more effective approaches for bi-level variable selection. Since DC (Difference of Convex functions) programming and DCA (DC Algorithm), powerful tools in nonconvex programming framework, have been successfully investigated for individual variable selection, we believe that they could be efficiently exploited for the more difficult bi-level variable selection task. In that direction, we investigate in this work DC approximations of the mixed zero norm L_0,0 and the combined norm L_0,0+L_q,0. The resulting approximate problems are then formulated as DC programs for which DCA based algorithms are introduced. As an application, these DCA schemes are developed for estimating multiple sparse covariance matrices sharing some common structures such as the locations or weights of non-zero elements. The experimental results on both simulated and real datasets indicate the efficiency of our algorithms.","tags":null,"title":"DCA based approaches for bi-level variable selection and application for estimate multiple sparse covariance matrices","type":"publication"},{"authors":["Hoai An Le Thi","Hoai Minh Le","Duy Nhat Phan","BachTran"],"categories":null,"content":"","date":1636934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636934400,"objectID":"aa57004696f560dd6be67cd3c536687c","permalink":"https://nhatpd.github.io/publication/jan_amc_2021/","publishdate":"2021-11-15T20:13:52.626042Z","relpermalink":"/publication/jan_amc_2021/","section":"publication","summary":"We address the problem of minimizing the sum of a nonconvex, differentiable function and composite functions by DC (Difference of Convex functions) programming and DCA (DC Algorithm), powerful tools of nonconvex optimization. The main idea of DCA relies on DC decompositions of the objective function, it consists in approximating a DC (nonconvex) program by a sequence of convex ones. We first develop a standard DCA scheme especially dealing with the very specific structure of this problem. Furthermore, we extend DCA to give rise to the so-named DCA-Like, which is based on a new and efficient way to approximate the DC objective function without knowing a DC decomposition. We further improve DCA based algorithms by incorporating the Nesterov’s acceleration technique into them. The convergence properties and the convergence rate under Kurdyka-Łojasiewicz assumption of extended DCAs are rigorously studied. We prove that DCA-Like and the accelerated versions subsequently converge from every initial point to a critical point of the considered problem. Finally, we investigate the proposed algorithms for an important problem in machine learning: the t-distributed stochastic neighbor embedding. Numerical experiments on several benchmark datasets illustrate the efficiency of our algorithms.","tags":null,"title":"Novel DCA based algorithms for a special class of nonconvex problems with application in machine learning","type":"publication"},{"authors":["Duy Nhat Phan","Thuy Ngoc Nguyen"],"categories":null,"content":"","date":1635797632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635797632,"objectID":"6384c567542ea494f2092dca485298c4","permalink":"https://nhatpd.github.io/publication/jnhat_cam_2021/","publishdate":"2021-11-01T20:13:52.626042Z","relpermalink":"/publication/jnhat_cam_2021/","section":"publication","summary":"Low-rank minimization problems arise in numerous important applications such as recommendation systems, machine learning, network analysis, and so on. The problems however typically consist of minimizing a sum of a smooth function and nonconvex nonsmooth composite functions, which solving them remains a big challenge. In this paper, we take inspiration from the Nesterov’s acceleration technique to accelerate an iteratively reweighted nuclear norm algorithm for the considered problems ensuring that every limit point is a critical point. Our algorithm iteratively computes the proximal operator of a reweighted nuclear norm which has a closed-form solution by performing the SVD of a smaller matrix instead of the full SVD. This distinguishes our work from recent accelerated proximal gradient algorithms that require an expensive computation of the proximal operator of nonconvex nonsmooth composite functions. We also investigate the convergence rate with the Kurdyka–Łojasiewicz assumption. Numerical experiments are performed to demonstrate the efficiency of our algorithm and its superiority over well-known methods.","tags":null,"title":"An accelerated IRNN-Iteratively Reweighted Nuclear Norm algorithm for nonconvex nonsmooth low-rank minimization problems","type":"publication"},{"authors":["Duy Nhat Phan","Hoai An Le Thi"],"categories":null,"content":"","date":1623196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623196800,"objectID":"08ca7167d59f2fee0c4e11eca5d2679b","permalink":"https://nhatpd.github.io/publication/pnhat_mor_2021/","publishdate":"2021-06-09T20:13:52.623034Z","relpermalink":"/publication/pnhat_mor_2021/","section":"publication","summary":"In this paper, we focus on the problem of minimizing the sum of a nonconvex differentiable function and a DC (Difference of Convex functions) function, where the differentiable function is not restricted to the global Lipschitz gradient continuity assumption. This problem covers a broad range of applications in machine learning and statistics such as compressed sensing, signal recovery, sparse dictionary learning, and matrix factorization, etc. We take inspiration from the Nesterov's acceleration technique and the DC algorithm to develop a novel algorithm for the considered problem. Analyzing the convergence, we study the subsequential convergence of our algorithm to a critical point. Furthermore, we justify the global convergence of the whole sequence generated by our algorithm to a critical point and establish its convergence rate under the Kurdyka-Lojasiewicz condition. Numerical experiments on the nonnegative matrix completion problem are performed to demonstrate the efficiency of our algorithm and its superiority over well-known methods.","tags":null,"title":"DCA based Algorithm with Extrapolation for Nonconvex Nonsmooth Optimization","type":"publication"},{"authors":["Thuy Ngoc Nguyen","Duy Nhat Phan","Cleotilde Gonzalez"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"d3cedcbebb3cc66cfd620ff9dab0d4a1","permalink":"https://nhatpd.github.io/publication/cngoc_acm_2021/","publishdate":"2021-06-05T20:13:52.632058Z","relpermalink":"/publication/cngoc_acm_2021/","section":"publication","summary":"","tags":null,"title":"A Cognitive Hysteretic-IBL Model for Coordinated Multi-Agent Transportation Problems","type":"publication"},{"authors":null,"categories":null,"content":"The High Profile Strikes Dataset (HPSD) is a coding of press reports catalogued in the “World Publications” section of the Nexis database pertaining to economic and political trade union protest. In the original dataset, the stories were identified by using broade search terms like \u0026ldquo;labor,\u0026rdquo; \u0026ldquo;strikes,\u0026rdquo; and \u0026ldquo;industrial disputes\u0026rdquo; and included protest from non-OECD countries for the period 1980 to 2005.\nThe new version of the dataset builds on the earlier version while also expanding the number of sources (e.g. moving beyond just Nexis). I am also looking for ways to make the database open-source, i.e. to allow individual researchers to contribute codings of strike events and to suggest corrections to earlier codings.\n","date":1621382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621382400,"objectID":"162ca3fb512b7c7c006c669e2c46b9ae","permalink":"https://nhatpd.github.io/project/high-profile-strikes/","publishdate":"2021-05-19T00:00:00Z","relpermalink":"/project/high-profile-strikes/","section":"project","summary":"A detailed coding of strike activity reported in major news outlets.","tags":["Strikes, Industrial Relations"],"title":"High Profile Strikes Dataset","type":"project"},{"authors":null,"categories":null,"content":"Data data everywhere and not a byte to analyze!\nEveryone who has done quantitative analysis of India\u0026rsquo;s political economy knows how frustrating it can be to wrangle the data. There are so much good data published by the Government of India, but it is frequently not released in a user-friendly format. The goal of this project is to rectify that problem by making state- and district-level data available in a more straightfoward and standardized way.\n","date":1621382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621382400,"objectID":"45be4a4cf7856b6e7d2fed88f4d4b744","permalink":"https://nhatpd.github.io/project/india-data/","publishdate":"2021-05-19T00:00:00Z","relpermalink":"/project/india-data/","section":"project","summary":"A repository for state and district-level data relating to the political economy of India.","tags":["India, Political Economy"],"title":"India Political Economy Data Repository","type":"project"},{"authors":null,"categories":null,"content":"This project aims to improve upon existing measures of labor standards, most of which evaluate a government\u0026rsquo;s respect for worker rights through observed violations of labor standards. I discuss this method and some shortcomings associated with it in an article that I published in Political Research Quarterly. The essential idea is to instead measure labor standards through evidence of the ability of workers to proactively exercise their rights.\nThe work involves gathering and standardizing cross-national data on union density, collective bargaining, worker protests and strikes. These data will then be used to develop a composite index of labor standards with statistical measurement models.\n","date":1621382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621382400,"objectID":"343c437646948d03cd393756913d7ef0","permalink":"https://nhatpd.github.io/project/measuring-labor-rights/","publishdate":"2021-05-19T00:00:00Z","relpermalink":"/project/measuring-labor-rights/","section":"project","summary":"An effort to improve cross-national measures of labor standards.","tags":["Labor Rights, Labor Standards"],"title":"Measuring Labor Rights","type":"project"},{"authors":null,"categories":null,"content":"This project aims to provide a detailed coding of violent events associated with South Asia\u0026rsquo;s many ongoing insurgencies. The primary source of data for these codings is the South Asia Terrorism Portal. So far, my team has coded more than ten thousand events related to India\u0026rsquo;s Maoist Insurgency occurring between 2005 and 2017. The next step will be to extend the coding of Maoist-related events to the present and then start coding events for other conflicts.\n","date":1616544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616544000,"objectID":"d5a384425f3b716b80a05a6b9dd5412f","permalink":"https://nhatpd.github.io/project/south-asia-conflict/","publishdate":"2021-03-24T00:00:00Z","relpermalink":"/project/south-asia-conflict/","section":"project","summary":"Who are the primary challengers to state authority in South Asia?","tags":["India, Maoist, Naxalite"],"title":"Conflict in South Asia","type":"project"},{"authors":["Hoai An Le Thi","Duy Nhat Phan","Tao Pham Dinh"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"0e42b33cedbe66704a45948dd53535c0","permalink":"https://nhatpd.github.io/publication/pan_jota_2021/","publishdate":"2021-01-01T20:13:52.623034Z","relpermalink":"/publication/pan_jota_2021/","section":"publication","summary":"Difference-of-Convex functions programming and Difference-of-Convex functions algorithm play a key role in nonsmooth nonconvex analysis/programming. For 35 years from their birthday, these theoretical and algorithmic tools have been greatly enriched, thanks to a lot of their applications, by researchers and practitioners in the world, to model and successfully solve nonconvex programs from many fields of applied sciences. Despite these bright successes, how to further exploit the full power and creative freedom offered by these tools to more advantageously improve the state of the art (the standard) Difference-of-Convex functions algorithm to better handle large scale problems and big data is still a great challenge in this field. This paper addresses the three most important key issues to meet this challenge: finding tailored Difference of Convex functions decompositions, speeding up convergence, and reducing the computational burden of the standard Difference-of-Convex functions algorithm. Several advanced algorithms are proposed: the successive Difference-of-Convex decomposition algorithm in which Difference-of-Convex components are successively changed, its inexact version, and the accelerated versions via Nesterov's acceleration technique of the two former approaches. Convergence properties of the proposed algorithms are carefully studied. We prove that, fortunately, in spite of these modifications, the proposed advanced algorithms have similar convergence properties to the standard algorithm. Moreover, we establish their convergence rates with Kurdyka-Lojasiewicz assumption. Finally, we demonstrate that various popular methods are special cases of the proposed algorithms.","tags":null,"title":"Advanced Difference-of-Convex functions Algorithms","type":"publication"},{"authors":["Hoai An Le Thi","Hoai Minh Le","Duy Nhat Phan","BachTran"],"categories":null,"content":"","date":1608063232,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608063232,"objectID":"7f34b968d0280f4caa0906a2426bf13e","permalink":"https://nhatpd.github.io/publication/jan_neura_2021/","publishdate":"2020-12-15T20:13:52.626042Z","relpermalink":"/publication/jan_neura_2021/","section":"publication","summary":"We consider the large sum of DC (Difference of Convex) functions minimization problem which appear in several different areas, especially in stochastic optimization and machine learning. Two DCA (DC Algorithm) based algorithms are proposed: stochastic DCA and inexact stochastic DCA. We prove that the convergence of both algorithms to a critical point is guaranteed with probability one. Furthermore, we develop our stochastic DCA for solving an important problem in multi-task learning, namely group variables selection in multi class logistic regression. The corresponding stochastic DCA is very inexpensive, all computations are explicit. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithms and their superiority over existing methods, with respect to classification accuracy, sparsity of solution as well as running time.","tags":null,"title":"Stochastic DCA for minimizing a large sum of DC functions with application to multi-class logistic regression","type":"publication"},{"authors":["Duy Nhat Phan","Hoai An Le Thi"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"38e550b4f5e80a7aa23f06548385798c","permalink":"https://nhatpd.github.io/publication/jnhat_neura_2019/","publishdate":"2019-10-01T20:13:52.626042Z","relpermalink":"/publication/jnhat_neura_2019/","section":"publication","summary":"The need to select groups of variables arises in many statistical modeling problems and applications. In this paper, we consider the -norm regularization for enforcing group sparsity and investigate a DC (Difference of Convex functions) approximation approach for solving the -norm regularization problem. We show that, with suitable parameters, the original and approximate problems are equivalent. Considering two equivalent formulations of the approximate problem we develop DC programming and DCA (DC Algorithm) for solving them. As an application, we implement the proposed algorithms for group variable selection in the optimal scoring problem. The sparsity is obtained by using the -regularization that selects the same features in all discriminant vectors. The resulting sparse discriminant vectors provide a more interpretable low-dimensional representation of data. The experimental results on both simulated datasets and real datasets indicate the efficiency of the proposed algorithms.","tags":null,"title":"Group variable selection via L_p,0 regularization and application to optimal scoring","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Key concepts and themes to review  colonialism partition communalism caste clientelism criminality patriarchy development     Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://nhatpd.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Duy Nhat Phan","Hoai Minh Le","Hoai An Le Thi"],"categories":null,"content":"","date":1531440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531440000,"objectID":"2c2c8cf758f51059298b71c694abcf8b","permalink":"https://nhatpd.github.io/publication/cnhat_ijcai_2018/","publishdate":"2018-07-13T20:13:52.632058Z","relpermalink":"/publication/cnhat_ijcai_2018/","section":"publication","summary":"In this work, we present a variant of DCA (Difference of Convex function Algorithm) with the aim of improving its performance. The proposed algorithm, named Accelerated DCA (ADCA), consists in incorporating the Nesterov’s acceleration technique into DCA. We first investigate ADCA for solving the standard DC program and rigorously study its convergence properties and the convergence rate. Secondly, we develop ADCA for a special case of the standard DC program whose the objective function is the sum of a differentiable function with L-Lipschitz continuous gradient (possibly nonconvex) and a DC function. We exploit the special structure of the problem to propose an efficient DC decomposition for which the corresponding ADCA scheme is inexpensive. As an application, we consider the sparse binary logistic regression problem. Numerical experiments on several benchmark datasets illustrate the efficiency of our algorithm and its superiority over well-known methods.","tags":null,"title":"Accelerated Difference of Convex functions Algorithm and its Application to Sparse Binary Logistic Regression","type":"publication"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"https://nhatpd.github.io/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"https://nhatpd.github.io/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"\u0026hellip;","tags":null,"title":"Terms","type":"page"},{"authors":["Duy Nhat Phan","Hoai An Le Thi","Tao Pham Dinh"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"cd2747c2a2e40211a5443cf3b7ea1509","permalink":"https://nhatpd.github.io/publication/jnhat_neura_2017/","publishdate":"2017-11-01T20:13:52.626042Z","relpermalink":"/publication/jnhat_neura_2017/","section":"publication","summary":"This letter proposes a novel approach using the -norm regularization for the sparse covariance matrix estimation (SCME) problem. The objective function of SCME problem is composed of a nonconvex part and the term, which is discontinuous and difficult to tackle. Appropriate DC (difference of convex functions) approximations of -norm are used that result in approximation SCME problems that are still nonconvex. DC programming and DCA (DC algorithm), powerful tools in nonconvex programming framework, are investigated. Two DC formulations are proposed and corresponding DCA schemes developed. Two applications of the SCME problem that are considered are classification via sparse quadratic discriminant analysis and portfolio optimization. A careful empirical experiment is performed through simulated and real data sets to study the performance of the proposed algorithms. Numerical results showed their efficiency and their superiority compared with seven state-of-the-art methods.","tags":null,"title":"Sparse Covariance Matrix Estimation by DCA-Based Algorithms","type":"publication"},{"authors":["Duy Nhat Phan","Hoai An Le Thi"],"categories":null,"content":"","date":1508371200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508371200,"objectID":"7db7e603e1eacc203857f34ad0ec5a3d","permalink":"https://nhatpd.github.io/publication/cnhat_dsaa_2017/","publishdate":"2017-10-19T20:13:52.632058Z","relpermalink":"/publication/cnhat_dsaa_2017/","section":"publication","summary":"We present a novel approach for estimating multiple sparse precision matrices using the L_0,0 regularization. The multiple precision matrices share some common structures such as the weights or locations of non-zero elements, we are interested in selecting simultaneously elements on each precision matrix as well as across multiple precision matrices. This is referred as bi-level variable selection. The optimization problem can be formulated as DC (Difference of Convex functions) programs to which DC programming and DCA (DC Algorithm) are investigated. The experimental results on both simulated and real datasets demonstrate the efficiency of our algorithms.","tags":null,"title":"A Novel Approach for Estimating Multiple Sparse Precision Matrices Using L_0,0 Regularization","type":"publication"},{"authors":["Hoai An Le Thi","Duy Nhat Phan"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"397de34e48cfb089cfc57e3e45464e93","permalink":"https://nhatpd.github.io/publication/jan_neura_2017/","publishdate":"2017-09-01T20:13:52.626042Z","relpermalink":"/publication/jan_neura_2017/","section":"publication","summary":"We consider the supervised pattern classification in the high-dimensional setting, in which the number of features is much larger than the number of observations. We present a novel approach to the sparse Fisher linear discriminant problem using the ℓ0-norm. The resulting optimization problem is nonconvex, discontinuous and very hard to solve. We overcome the discontinuity by using appropriate approximations to the ℓ0-norm such that the resulting problems can be formulated as difference of convex functions (DC) programs to which DC programming and DC Algorithms (DCA) are investigated. The experimental results on both simulated and real datasets demonstrate the efficiency of the proposed algorithms compared to some state-of-the-art methods.","tags":null,"title":"DC programming and DCA for sparse Fisher linear discriminant analysis","type":"publication"},{"authors":["Hoai An Le Thi","Hoai Minh Le","Duy Nhat Phan","Bach Tran"],"categories":null,"content":"","date":1500249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500249600,"objectID":"923b97028aba2b770aa73c14d5a7601d","permalink":"https://nhatpd.github.io/publication/can_icml_2017/","publishdate":"2017-07-17T20:13:52.632058Z","relpermalink":"/publication/can_icml_2017/","section":"publication","summary":"In this paper, we present a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems whose objective function is a large sum of non-convex functions and a regularization term. We consider the  regularization to deal with the group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. As an application, we applied our algorithm for the group variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.","tags":null,"title":"Stochastic DCA for the large-sum of non-convex functions problem and its application to group variable selection in classification","type":"publication"},{"authors":["Duy Nhat Phan","Hoai An Le Thi","Tao Pham Dinh"],"categories":null,"content":"","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495670400,"objectID":"d4e36f593a7d2a02a4ea1623046fd524","permalink":"https://nhatpd.github.io/publication/cnhat_pakdd_2017/","publishdate":"2017-05-25T20:13:52.632058Z","relpermalink":"/publication/cnhat_pakdd_2017/","section":"publication","summary":"Variable selection plays an important role in analyzing high dimensional data. When the data possesses certain group structures in which individual variables are also meaningful scientifically, we are naturally interested in selecting important groups as well as important variables. We introduce a new regularization by combining the -norm and -norm for bi-level variable selection. Using an appropriate DC (Difference of Convex functions) approximation, the resulting problem can be solved by DC Algorithm. As an application, we implement the proposed algorithm for estimating multiple covariance matrices sharing some common structures such as the locations or weights of non-zero elements. The experimental results on both simulated and real datasets demonstrate the efficiency of our algorithm.","tags":null,"title":"Efficient bi-level variable selection and application to estimation of multiple covariance matrices","type":"publication"},{"authors":["Hoai An Le Thi","Hoai Minh Le","Duy Nhat Phan","Bach Tran"],"categories":null,"content":"","date":1495670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495670400,"objectID":"c104874963f63e6ae195b7d556a12afa","permalink":"https://nhatpd.github.io/publication/can_iccsama_2017/","publishdate":"2017-05-25T20:13:52.632058Z","relpermalink":"/publication/can_iccsama_2017/","section":"publication","summary":"In this paper, we deal with the multiclass logistic regression problem, one of the most popular supervised classification method. We aim at developing an efficient method to solve this problem for large-scale datasets, i.e. large number of features and large number of instances. To deal with a large number of features, we consider feature selection method evolving the  regularization. The resulting optimization problem is non-convex for which we develop a stochastic version of DCA (Difference of Convex functions Algorithm) to solve. This approach is suitable to handle datasets with very large number of instances. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.","tags":null,"title":"Stochastic DCA for Sparse Multiclass Logistic Regression","type":"publication"},{"authors":["Hoai An Le Thi","Duy Nhat Phan"],"categories":null,"content":"","date":1461024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461024000,"objectID":"d1943feb205d20a96239a287f09996cc","permalink":"https://nhatpd.github.io/publication/jan_neuro_2016/","publishdate":"2016-04-19T20:13:52.626042Z","relpermalink":"/publication/jan_neuro_2016/","section":"publication","summary":"Linear Discriminant Analysis (LDA) is a standard tool for classification and dimension reduction in many applications. However, the problem of high dimension is still a great challenge for the classical LDA. In this paper we consider the supervised pattern classification in the high dimensional setting, in which the number of features is much larger than the number of observations and present a novel approach to the sparse optimal scoring problem using the zero-norm. The difficulty in treating the zero-norm is overcome by using appropriate continuous approximations such that the resulting problems are solved by alternating schemes based on DC (Difference of Convex functions) programming and DCA (DC Algorithms). The experimental results on both simulated and real datasets show the efficiency of the proposed algorithms compared to the five state-of-the-art methods.","tags":null,"title":"DC programming and DCA for sparse optimal scoring problem","type":"publication"},{"authors":["Hoai An Le Thi","Duy Nhat Phan"],"categories":null,"content":"","date":1431993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431993600,"objectID":"39776494da468811f7f6947c61b3c466","permalink":"https://nhatpd.github.io/publication/can_pakdd_2015/","publishdate":"2015-05-19T20:13:52.632058Z","relpermalink":"/publication/can_pakdd_2015/","section":"publication","summary":"We consider the supervised classification problem in the high-dimensional setting. High-dimensionality makes the application of most classification difficult. We present a novel approach to the sparse linear discriminant analysis (LDA) based on its optimal scoring interpretation and the zero-norm. The difficulty in treating the zero-norm is overcome by using an appropriate continuous approximation such that the resulting problem can be formulated as a DC (Difference of Convex functions) program to which DCA (DC Algorithms) is investigated. The computational results on both simulated data and real microarray cancer data show the efficiency of the proposed algorithm in feature selection as well as classification.","tags":null,"title":"A DC Programming Approach for Sparse Optimal Scoring","type":"publication"},{"authors":["Duy Nhat Phan","Hoai An Le Thi","Tao Pham Dinh"],"categories":null,"content":"","date":1429401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429401600,"objectID":"1c96e163d8adb13df4056c22118ead0c","permalink":"https://nhatpd.github.io/publication/cnhat_cmo_205/","publishdate":"2015-04-19T20:13:52.632058Z","relpermalink":"/publication/cnhat_cmo_205/","section":"publication","summary":"We suggest a novel approach to the sparse covariance matrix estimation (SCME) problem using the ℓ1-norm. The resulting optimization problem is nonconvex and very hard to solve. Fortunately, it can be reformulated as DC (Difference of Convex functions) programs to which DC programming and DC Algorithms can be investigated. The main contribution of this paper is to propose a more suitable DC decomposition for solving the SCME problem. The experimental results on both simulated datasets and two real datasets in classification problem illustrate the efficiency of the proposed algorithms.","tags":null,"title":"A DC Programming Approach for Sparse Estimation of a Covariance Matrix","type":"publication"},{"authors":["Duy Nhat Phan","Manh Cuong Nguyen","Hoai An Le Thi"],"categories":null,"content":"","date":1397865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397865600,"objectID":"1823df541fa577c90619bf5350cb7e9e","permalink":"https://nhatpd.github.io/publication/cnhat_iccsama_2014/","publishdate":"2014-04-19T20:13:52.632058Z","relpermalink":"/publication/cnhat_iccsama_2014/","section":"publication","summary":"We consider the supervised pattern classification in the high-dimensional setting, in which the number of features is much larger than the number of observations. We present a novel approach to the sparse linear discriminant analysis (LDA) using the zero-norm. The resulting optimization problem is non-convex, discontinuous and very hard to solve. We overcome the discontinuity by using an appropriate continuous approximation to zero-norm such that the resulting problem can be formulated as a DC (Difference of Convex functions) program to which DC programming and DC Algorithms (DCA) can be investigated. The computational results show the efficiency and the superiority of our approach versus the l 1 regularization model on both feature selection and classification.","tags":null,"title":"A dc programming approach for sparse linear discriminant analysis","type":"publication"}]